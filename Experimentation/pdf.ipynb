{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "file_name = \"\"\"C:\\\\Users\\\\mikke\\\\OneDrive\\\\Dokumenter\\\\DTU documents\\\\10. semester (kandidat 2. semester)\\\\02465 - Introduction to Reinforcement Learning and Control\\\\sutton2018.pdf\"\"\"\n",
    "loader = PyPDFLoader(file_name)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.9. Heuristic Search 181\n",
      "expression, needed to select the action for many states, including St. Used this way,\n",
      "planning is not focused on the current state. We call planning used in this way background\n",
      "planning .\n",
      "The other way to use planning is to begin and complete it after encountering each\n",
      "new state St, as a computation whose output is the selection of a single action At; on\n",
      "the next step planning begins anew with St+1to produce At+1, and so on. The simplest,\n",
      "and almost degenerate, example of this use of planning is when only state values are\n",
      "available, and an action is selected by comparing the values of model-predicted next states\n",
      "for each action (or by comparing the values of afterstates as in the tic-tac-toe example\n",
      "in Chapter 1). More generally, planning used in this way can look much deeper than\n",
      "one-step-ahead and evaluate action choices leading to many di↵erent predicted state and\n",
      "reward trajectories. Unlike the ﬁrst use of planning, here planning focuses on a particular\n",
      "state. We call this decision-time planning .\n",
      "These two ways of thinking about planning—using simulated experience to gradually\n",
      "improve a policy or value function, or using simulated experience to select an action for\n",
      "the current state—can blend together in natural and interesting ways, but they have\n",
      "tended to be studied separately, and that is a good way to ﬁrst understand them. Let us\n",
      "now take a closer look at decision-time planning.\n",
      "Even when planning is only done at decision time, we can still view it, as we did\n",
      "in Section 8.1, as proceeding from simulated experience to updates and values, and\n",
      "ultimately to a policy. It is just that now the values and policy are speciﬁc to the current\n",
      "state and the action choices available there, so much so that the values and policy created\n",
      "by the planning process are typically discarded after being used to select the current\n",
      "action. In many applications this is not a great loss because there are very many states\n",
      "and we are unlikely to return to the same state for a long time. In general, one may\n",
      "want to do a mix of both: focus planning on the current state and store the results\n",
      "of planning so as to be that much farther along should one return to the same state\n",
      "later. Decision-time planning is most useful in applications in which fast responses are\n",
      "not required. In chess playing programs, for example, one may be permitted seconds or\n",
      "minutes of computation for each move, and strong programs may plan dozens of moves\n",
      "ahead within this time. On the other hand, if low latency action selection is the priority,\n",
      "then one is generally better o↵ doing planning in the background to compute a policy\n",
      "that can then be rapidly applied to each newly encountered state.\n",
      "8.9 Heuristic Search\n",
      "The classical state-space planning methods in artiﬁcial intelligence are decision-time\n",
      "planning methods collectively known as heuristic search . In heuristic search, for each\n",
      "state encountered, a large tree of possible continuations is considered. The approximate\n",
      "value function is applied to the leaf nodes and then backed up toward the current state\n",
      "at the root. The backing up within the search tree is just the same as in the expected\n",
      "updates with maxes (those for v⇤andq⇤) discussed throughout this book. The backing\n",
      "up stops at the state–action nodes for the current state. Once the backed-up values of\n",
      "these nodes are computed, the best of them is chosen as the current action, and then all\n"
     ]
    }
   ],
   "source": [
    "print(pages[200].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
